/home/shrikant/miniconda3/bin/conda run -n averitec_DESK1 --no-capture-output python src/my_methods/roberta_sentence_selector/train_roberta_sentence_selector.py --data_dir data --bert_name roberta-base --max_epoch 10 --user_given_model_suffix sent_reranker 
Namespace(data_dir='data', cache_dir=None, ckpt_root_dir='bert_weights/roberta_sentence_selector_ckpt_root_dir', test_ckpt=None, load_model_path=None, model_type='RobertaCls', bert_name='roberta-base', data_generator='RobertaSentenceGenerator', force_generate=False, save_all_ckpt=False, test=False, fix_bert=False, device='cuda:0', batch_size=16, gradient_accumulation_steps=4, dropout=0.5, warm_rate=0.2, seed=1234, lr=1e-05, weight_decay=0.0001, max_seq_len=512, max_epoch=10, print_freq=100, save_freq=1, wiki_path='data/feverous_wikiv1.db', predict_threshold=0.5, max_sent=5, retrieval_turns=3, user_given_model_suffix='sent_reranker')
/home/shrikant/miniconda3/envs/averitec_DESK1/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
0 -> init linear1.weight torch.Size([128, 768])
1 -> not init linear1.bias torch.Size([128])
2 -> init linear2.weight torch.Size([1, 128])
3 -> not init linear2.bias torch.Size([1])
Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
The model has 124,744,193 trainable parameters
generate 50000 sentence pairs
generate 8479 sentence pairs
train data length: 8479
dev data length: 50000
test data length: 0
bert parameters
n: bert.embeddings.word_embeddings.weight, shape: torch.Size([50265, 768])
n: bert.embeddings.position_embeddings.weight, shape: torch.Size([514, 768])
n: bert.embeddings.token_type_embeddings.weight, shape: torch.Size([1, 768])
n: bert.embeddings.LayerNorm.weight, shape: torch.Size([768])
n: bert.embeddings.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.0.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.0.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.0.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.0.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.0.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.0.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.0.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.0.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.0.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.0.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.0.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.0.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.0.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.0.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.0.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.0.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.1.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.1.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.1.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.1.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.1.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.1.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.1.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.1.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.1.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.1.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.1.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.1.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.1.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.1.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.1.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.1.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.2.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.2.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.2.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.2.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.2.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.2.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.2.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.2.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.2.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.2.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.2.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.2.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.2.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.2.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.2.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.2.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.3.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.3.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.3.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.3.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.3.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.3.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.3.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.3.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.3.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.3.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.3.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.3.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.3.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.3.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.3.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.3.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.4.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.4.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.4.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.4.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.4.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.4.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.4.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.4.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.4.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.4.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.4.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.4.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.4.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.4.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.4.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.4.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.5.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.5.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.5.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.5.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.5.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.5.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.5.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.5.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.5.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.5.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.5.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.5.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.5.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.5.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.5.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.5.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.6.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.6.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.6.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.6.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.6.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.6.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.6.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.6.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.6.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.6.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.6.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.6.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.6.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.6.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.6.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.6.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.7.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.7.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.7.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.7.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.7.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.7.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.7.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.7.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.7.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.7.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.7.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.7.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.7.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.7.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.7.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.7.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.8.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.8.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.8.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.8.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.8.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.8.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.8.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.8.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.8.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.8.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.8.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.8.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.8.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.8.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.8.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.8.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.9.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.9.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.9.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.9.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.9.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.9.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.9.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.9.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.9.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.9.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.9.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.9.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.9.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.9.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.9.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.9.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.10.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.10.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.10.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.10.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.10.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.10.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.10.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.10.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.10.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.10.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.10.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.10.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.10.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.10.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.10.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.10.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.11.attention.self.query.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.11.attention.self.query.bias, shape: torch.Size([768])
n: bert.encoder.layer.11.attention.self.key.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.11.attention.self.key.bias, shape: torch.Size([768])
n: bert.encoder.layer.11.attention.self.value.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.11.attention.self.value.bias, shape: torch.Size([768])
n: bert.encoder.layer.11.attention.output.dense.weight, shape: torch.Size([768, 768])
n: bert.encoder.layer.11.attention.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.11.attention.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.11.attention.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.encoder.layer.11.intermediate.dense.weight, shape: torch.Size([3072, 768])
n: bert.encoder.layer.11.intermediate.dense.bias, shape: torch.Size([3072])
n: bert.encoder.layer.11.output.dense.weight, shape: torch.Size([768, 3072])
n: bert.encoder.layer.11.output.dense.bias, shape: torch.Size([768])
n: bert.encoder.layer.11.output.LayerNorm.weight, shape: torch.Size([768])
n: bert.encoder.layer.11.output.LayerNorm.bias, shape: torch.Size([768])
n: bert.pooler.dense.weight, shape: torch.Size([768, 768])
n: bert.pooler.dense.bias, shape: torch.Size([768])
******************************************************************************************************************************************************
task parameters
n: linear1.weight, shape: torch.Size([128, 768])
n: linear1.bias, shape: torch.Size([128])
n: linear2.weight, shape: torch.Size([1, 128])
n: linear2.bias, shape: torch.Size([1])
/home/shrikant/miniconda3/envs/averitec_DESK1/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
generate 8479 sentence pairs
0it [00:00, ?it/s]Epoch:0,step:0
Train Loss:0.002541    Acccuracy:0.406   Precision:0.333333   Recall 0.187500
100it [00:36,  2.69it/s]Epoch:0,step:100
Train Loss:0.250428    Acccuracy:49.844   Precision:53.425397   Recall 13.687500
200it [01:13,  2.69it/s]Epoch:0,step:200
Train Loss:0.250688    Acccuracy:49.906   Precision:94.000000   Recall 0.250000
300it [01:50,  2.69it/s]Epoch:0,step:300
Train Loss:0.247223    Acccuracy:50.281   Precision:57.434127   Recall 8.062500
400it [02:27,  2.69it/s]Epoch:0,step:400
Train Loss:0.183377    Acccuracy:65.062   Precision:64.605218   Recall 67.937500
500it [03:04,  2.69it/s]Epoch:0,step:500
Train Loss:0.163477    Acccuracy:67.625   Precision:66.205873   Recall 74.187500
530it [03:15,  2.71it/s]
====train step of epoch 0 ==========
train_epoch0_loss: 0.8636046424226941
train_epoch0_accuracy: 0.5714706922986201
train_epoch0_precision: 0.6280101394169835
train_epoch0_recall: 0.3506309706333294
              precision    recall  f1-score   support

      NOTSEL       0.55      0.79      0.65      8479
      SELECT       0.63      0.35      0.45      8479

    accuracy                           0.57     16958
   macro avg       0.59      0.57      0.55     16958
weighted avg       0.59      0.57      0.55     16958

0.5714706922986201 0.5714706922986201 0.5714706922986201 0.5714706922986201
====validation step of epoch 0======================
3125it [03:17, 15.80it/s]
Answer-only score metric=(HU-meteor) level=5 : 0.12223773601309366
Answer-only score metric=(HU-meteor) level=10 : 0.1491199851273267
Answer-only score metric=(HU-meteor) level=50 : 0.21288083127505836
Answer-only score metric=(HU-meteor) level=100 : 0.23383160141118098
save to checkpoint: bert_weights/roberta_sentence_selector_ckpt_root_dir/RobertaCls_0518_22:24:57_sent_reranker
generate 8479 sentence pairs
0it [00:00, ?it/s]Epoch:1,step:0
Train Loss:0.001059    Acccuracy:0.812   Precision:0.750000   Recall 0.937500
100it [00:36,  2.73it/s]Epoch:1,step:100
Train Loss:0.160837    Acccuracy:68.031   Precision:69.561735   Recall 68.875000
200it [01:13,  2.72it/s]Epoch:1,step:200
Train Loss:0.160455    Acccuracy:68.000   Precision:65.983426   Recall 76.312500
300it [01:50,  2.73it/s]Epoch:1,step:300
Train Loss:0.168304    Acccuracy:66.344   Precision:60.802701   Recall 93.625000
400it [02:27,  2.73it/s]Epoch:1,step:400
Train Loss:0.159393    Acccuracy:68.156   Precision:64.328148   Recall 83.500000
500it [03:03,  2.73it/s]Epoch:1,step:500
Train Loss:0.158986    Acccuracy:68.344   Precision:69.455327   Recall 69.250000
530it [03:14,  2.72it/s]
====train step of epoch 1 ==========
train_epoch1_loss: 0.644094289361306
train_epoch1_accuracy: 0.6790305460549593
train_epoch1_precision: 0.6486486486486487
train_epoch1_recall: 0.7812242009670952
              precision    recall  f1-score   support

      NOTSEL       0.73      0.58      0.64      8479
      SELECT       0.65      0.78      0.71      8479

    accuracy                           0.68     16958
   macro avg       0.69      0.68      0.68     16958
weighted avg       0.69      0.68      0.68     16958

0.6790305460549593 0.6790305460549593 0.6790305460549593 0.6790305460549593
====validation step of epoch 1======================
3125it [03:17, 15.79it/s]
Answer-only score metric=(HU-meteor) level=5 : 0.12556135128802293
Answer-only score metric=(HU-meteor) level=10 : 0.14659630994513523
Answer-only score metric=(HU-meteor) level=50 : 0.21202275336362336
Answer-only score metric=(HU-meteor) level=100 : 0.23383160141118098
generate 8479 sentence pairs
0it [00:00, ?it/s]Epoch:2,step:0
Train Loss:0.001905    Acccuracy:0.625   Precision:0.642857   Recall 0.562500
100it [00:36,  2.70it/s]Epoch:2,step:100
Train Loss:0.159290    Acccuracy:68.125   Precision:66.709247   Recall 74.812500
200it [01:13,  2.71it/s]Epoch:2,step:200
Train Loss:0.154926    Acccuracy:68.875   Precision:69.669707   Recall 67.937500
300it [01:49,  2.70it/s]Epoch:2,step:300
Train Loss:0.149234    Acccuracy:70.156   Precision:71.220660   Recall 68.562500
400it [02:26,  2.71it/s]Epoch:2,step:400
Train Loss:0.147243    Acccuracy:70.562   Precision:71.742212   Recall 69.187500
500it [03:03,  2.71it/s]Epoch:2,step:500
Train Loss:0.161809    Acccuracy:67.656   Precision:68.307718   Recall 67.625000
530it [03:13,  2.73it/s]
====train step of epoch 2 ==========
train_epoch2_loss: 0.6172265330294393
train_epoch2_accuracy: 0.6911192357589339
train_epoch2_precision: 0.6897318815127035
train_epoch2_recall: 0.6947753272791602
              precision    recall  f1-score   support

      NOTSEL       0.69      0.69      0.69      8479
      SELECT       0.69      0.69      0.69      8479

    accuracy                           0.69     16958
   macro avg       0.69      0.69      0.69     16958
weighted avg       0.69      0.69      0.69     16958

0.6911192357589339 0.6911192357589339 0.6911192357589339 0.6911192357589339
====validation step of epoch 2======================
3125it [03:18, 15.78it/s]
Answer-only score metric=(HU-meteor) level=5 : 0.12301846883742644
Answer-only score metric=(HU-meteor) level=10 : 0.14889565004990538
Answer-only score metric=(HU-meteor) level=50 : 0.207936881293479
Answer-only score metric=(HU-meteor) level=100 : 0.23383160141118098
generate 8479 sentence pairs
0it [00:00, ?it/s]Epoch:3,step:0
Train Loss:0.001406    Acccuracy:0.719   Precision:0.705882   Recall 0.750000
100it [00:36,  2.75it/s]Epoch:3,step:100
Train Loss:0.151783    Acccuracy:69.625   Precision:70.614004   Recall 68.812500
200it [01:13,  2.74it/s]Epoch:3,step:200
Train Loss:0.151805    Acccuracy:69.656   Precision:71.090046   Recall 67.250000
300it [01:49,  2.75it/s]Epoch:3,step:300
Train Loss:0.151577    Acccuracy:69.656   Precision:70.948211   Recall 68.000000
400it [02:26,  2.74it/s]Epoch:3,step:400
Train Loss:0.155924    Acccuracy:68.812   Precision:71.173382   Recall 64.625000
500it [03:02,  2.75it/s]Epoch:3,step:500
Train Loss:0.150984    Acccuracy:69.781   Precision:70.854848   Recall 67.812500
530it [03:13,  2.74it/s]
====train step of epoch 3 ==========
train_epoch3_loss: 0.6084940690758094
train_epoch3_accuracy: 0.6956598655501828
train_epoch3_precision: 0.7039586919104991
train_epoch3_recall: 0.6753154853166647
              precision    recall  f1-score   support

      NOTSEL       0.69      0.72      0.70      8479
      SELECT       0.70      0.68      0.69      8479

    accuracy                           0.70     16958
   macro avg       0.70      0.70      0.70     16958
weighted avg       0.70      0.70      0.70     16958

0.6956598655501828 0.6956598655501828 0.6956598655501828 0.6956598655501828
====validation step of epoch 3======================
3125it [03:18, 15.76it/s]
Answer-only score metric=(HU-meteor) level=5 : 0.12355077217468369
Answer-only score metric=(HU-meteor) level=10 : 0.15184265854995693
Answer-only score metric=(HU-meteor) level=50 : 0.21162142002867532
Answer-only score metric=(HU-meteor) level=100 : 0.23383160141118098
save to checkpoint: bert_weights/roberta_sentence_selector_ckpt_root_dir/RobertaCls_0518_22:24:57_sent_reranker
generate 8479 sentence pairs
0it [00:00, ?it/s]Epoch:4,step:0
Train Loss:0.001562    Acccuracy:0.688   Precision:0.666667   Recall 0.750000
100it [00:36,  2.71it/s]Epoch:4,step:100
Train Loss:0.157876    Acccuracy:68.312   Precision:67.519027   Recall 72.437500
200it [01:12,  2.71it/s]Epoch:4,step:200
Train Loss:0.160779    Acccuracy:67.906   Precision:64.713514   Recall 82.187500
300it [01:49,  2.71it/s]Epoch:4,step:300
Train Loss:0.156136    Acccuracy:68.875   Precision:69.942610   Recall 67.562500
400it [02:26,  2.71it/s]Epoch:4,step:400
Train Loss:0.148724    Acccuracy:70.281   Precision:70.693560   Recall 70.562500
500it [03:02,  2.71it/s]Epoch:4,step:500
Train Loss:0.143336    Acccuracy:71.406   Precision:71.336291   Recall 72.937500
530it [03:13,  2.74it/s]
====train step of epoch 4 ==========
train_epoch4_loss: 0.6137096706707522
train_epoch4_accuracy: 0.6933600660455242
train_epoch4_precision: 0.6799473164306882
train_epoch4_recall: 0.7306286118646067
              precision    recall  f1-score   support

      NOTSEL       0.71      0.66      0.68      8479
      SELECT       0.68      0.73      0.70      8479

    accuracy                           0.69     16958
   macro avg       0.69      0.69      0.69     16958
weighted avg       0.69      0.69      0.69     16958

0.6933600660455242 0.6933600660455242 0.6933600660455242 0.6933600660455242
====validation step of epoch 4======================
3125it [03:18, 15.78it/s]
Answer-only score metric=(HU-meteor) level=5 : 0.12277475568596691
Answer-only score metric=(HU-meteor) level=10 : 0.15125829172890326
Answer-only score metric=(HU-meteor) level=50 : 0.2150983425161328
Answer-only score metric=(HU-meteor) level=100 : 0.23383160141118098
generate 8479 sentence pairs
0it [00:00, ?it/s]Epoch:5,step:0
Train Loss:0.002117    Acccuracy:0.562   Precision:0.562500   Recall 0.562500
100it [00:36,  2.76it/s]Epoch:5,step:100
Train Loss:0.158730    Acccuracy:68.156   Precision:64.116012   Recall 85.125000
200it [01:12,  2.76it/s]Epoch:5,step:200
Train Loss:0.163984    Acccuracy:67.188   Precision:62.379276   Recall 88.687500
300it [01:49,  2.75it/s]Epoch:5,step:300
Train Loss:0.164418    Acccuracy:67.062   Precision:62.322467   Recall 88.187500
400it [02:25,  2.75it/s]Epoch:5,step:400
Train Loss:0.153539    Acccuracy:69.250   Precision:65.779295   Recall 83.875000
500it [03:02,  2.75it/s]Epoch:5,step:500
Train Loss:0.152242    Acccuracy:69.562   Precision:68.259645   Recall 74.625000
530it [03:13,  2.74it/s]
====train step of epoch 5 ==========
train_epoch5_loss: 0.6337811946306589
train_epoch5_accuracy: 0.6825097299209812
train_epoch5_precision: 0.639678671360231
train_epoch5_recall: 0.8358296968982192
              precision    recall  f1-score   support

      NOTSEL       0.76      0.53      0.63      8479
      SELECT       0.64      0.84      0.72      8479

    accuracy                           0.68     16958
   macro avg       0.70      0.68      0.67     16958
weighted avg       0.70      0.68      0.67     16958

0.6825097299209812 0.6825097299209812 0.6825097299209812 0.6825097299209812
====validation step of epoch 5======================
3125it [03:17, 15.78it/s]
Answer-only score metric=(HU-meteor) level=5 : 0.124611480289596
Answer-only score metric=(HU-meteor) level=10 : 0.1496620022696515
Answer-only score metric=(HU-meteor) level=50 : 0.21265250931951482
Answer-only score metric=(HU-meteor) level=100 : 0.23383160141118098
generate 8479 sentence pairs
0it [00:00, ?it/s]Epoch:6,step:0
Train Loss:0.002024    Acccuracy:0.594   Precision:0.588235   Recall 0.625000
100it [00:36,  2.71it/s]Epoch:6,step:100
Train Loss:0.158944    Acccuracy:68.250   Precision:67.313680   Recall 72.875000
200it [01:13,  2.71it/s]Epoch:6,step:200
Train Loss:0.154251    Acccuracy:69.156   Precision:68.155989   Recall 73.750000
300it [01:49,  2.70it/s]Epoch:6,step:300
Train Loss:0.150692    Acccuracy:69.906   Precision:69.345755   Recall 71.875000
400it [02:26,  2.70it/s]Epoch:6,step:400
Train Loss:0.151354    Acccuracy:69.750   Precision:68.735581   Recall 73.687500
500it [03:02,  2.70it/s]Epoch:6,step:500
Train Loss:0.154286    Acccuracy:69.156   Precision:70.194460   Recall 67.875000
530it [03:13,  2.73it/s]
====train step of epoch 6 ==========
train_epoch6_loss: 0.6155012104589984
train_epoch6_accuracy: 0.6924755277745017
train_epoch6_precision: 0.6828366569572036
train_epoch6_recall: 0.718834768250973
              precision    recall  f1-score   support

      NOTSEL       0.70      0.67      0.68      8479
      SELECT       0.68      0.72      0.70      8479

    accuracy                           0.69     16958
   macro avg       0.69      0.69      0.69     16958
weighted avg       0.69      0.69      0.69     16958

0.6924755277745017 0.6924755277745017 0.6924755277745017 0.6924755277745017
====validation step of epoch 6======================
3125it [03:17, 15.79it/s]
Answer-only score metric=(HU-meteor) level=5 : 0.12550052759032745
Answer-only score metric=(HU-meteor) level=10 : 0.15217794236356783
Answer-only score metric=(HU-meteor) level=50 : 0.2109958466112089
Answer-only score metric=(HU-meteor) level=100 : 0.23383160141118098
save to checkpoint: bert_weights/roberta_sentence_selector_ckpt_root_dir/RobertaCls_0518_22:24:57_sent_reranker
generate 8479 sentence pairs
0it [00:00, ?it/s]Epoch:7,step:0
Train Loss:0.001563    Acccuracy:0.688   Precision:0.687500   Recall 0.687500
100it [00:36,  2.75it/s]Epoch:7,step:100
Train Loss:0.155748    Acccuracy:68.906   Precision:69.433897   Recall 68.625000
200it [01:13,  2.75it/s]Epoch:7,step:200
Train Loss:0.152926    Acccuracy:69.406   Precision:69.466673   Recall 70.750000
300it [01:49,  2.74it/s]Epoch:7,step:300
Train Loss:0.152574    Acccuracy:69.500   Precision:69.527814   Recall 70.437500
400it [02:26,  2.75it/s]Epoch:7,step:400
Train Loss:0.150092    Acccuracy:69.969   Precision:69.349107   Recall 72.687500
500it [03:02,  2.75it/s]Epoch:7,step:500
Train Loss:0.150525    Acccuracy:69.875   Precision:69.855149   Recall 70.812500
530it [03:13,  2.74it/s]
====train step of epoch 7 ==========
train_epoch7_loss: 0.6084787897906213
train_epoch7_accuracy: 0.6957778039863192
train_epoch7_precision: 0.6912883152800184
train_epoch7_recall: 0.7075126783818847
              precision    recall  f1-score   support

      NOTSEL       0.70      0.68      0.69      8479
      SELECT       0.69      0.71      0.70      8479

    accuracy                           0.70     16958
   macro avg       0.70      0.70      0.70     16958
weighted avg       0.70      0.70      0.70     16958

0.6957778039863192 0.6957778039863192 0.6957778039863192 0.6957778039863192
====validation step of epoch 7======================
3125it [03:17, 15.79it/s]
Answer-only score metric=(HU-meteor) level=5 : 0.12455515835587673
Answer-only score metric=(HU-meteor) level=10 : 0.1509230091767883
Answer-only score metric=(HU-meteor) level=50 : 0.21068931720876305
Answer-only score metric=(HU-meteor) level=100 : 0.23383160141118098
generate 8479 sentence pairs
0it [00:00, ?it/s]Epoch:8,step:0
Train Loss:0.001719    Acccuracy:0.656   Precision:0.631579   Recall 0.750000
100it [00:36,  2.71it/s]Epoch:8,step:100
Train Loss:0.153110    Acccuracy:69.375   Precision:69.658038   Recall 69.500000
200it [01:13,  2.71it/s]Epoch:8,step:200
Train Loss:0.153432    Acccuracy:69.250   Precision:69.182827   Recall 71.312500
300it [01:49,  2.72it/s]Epoch:8,step:300
Train Loss:0.141177    Acccuracy:71.750   Precision:71.785627   Recall 73.312500
400it [02:26,  2.72it/s]Epoch:8,step:400
Train Loss:0.148291    Acccuracy:70.469   Precision:70.619843   Recall 71.312500
500it [03:02,  2.71it/s]Epoch:8,step:500
Train Loss:0.155775    Acccuracy:68.844   Precision:69.567584   Recall 69.062500
530it [03:13,  2.74it/s]
====train step of epoch 8 ==========
train_epoch8_loss: 0.6020763175105149
train_epoch8_accuracy: 0.6990211109800684
train_epoch8_precision: 0.6947040498442367
train_epoch8_recall: 0.710107323976884
              precision    recall  f1-score   support

      NOTSEL       0.70      0.69      0.70      8479
      SELECT       0.69      0.71      0.70      8479

    accuracy                           0.70     16958
   macro avg       0.70      0.70      0.70     16958
weighted avg       0.70      0.70      0.70     16958

0.6990211109800684 0.6990211109800684 0.6990211109800684 0.6990211109800684
====validation step of epoch 8======================
3125it [03:17, 15.79it/s]
Answer-only score metric=(HU-meteor) level=5 : 0.12497868597456696
Answer-only score metric=(HU-meteor) level=10 : 0.15195840662385976
Answer-only score metric=(HU-meteor) level=50 : 0.21217249005543307
Answer-only score metric=(HU-meteor) level=100 : 0.23383160141118098
generate 8479 sentence pairs
0it [00:00, ?it/s]Epoch:9,step:0
Train Loss:0.002031    Acccuracy:0.594   Precision:0.588235   Recall 0.625000
100it [00:36,  2.75it/s]Epoch:9,step:100
Train Loss:0.150068    Acccuracy:69.938   Precision:69.877298   Recall 71.500000
200it [01:13,  2.75it/s]Epoch:9,step:200
Train Loss:0.156466    Acccuracy:68.625   Precision:68.974563   Recall 68.687500
300it [01:49,  2.75it/s]Epoch:9,step:300
Train Loss:0.148119    Acccuracy:70.344   Precision:70.297421   Recall 71.812500
400it [02:26,  2.74it/s]Epoch:9,step:400
Train Loss:0.148927    Acccuracy:70.219   Precision:69.754028   Recall 72.750000
500it [03:02,  2.75it/s]Epoch:9,step:500
Train Loss:0.147491    Acccuracy:70.500   Precision:70.231775   Recall 71.812500
530it [03:13,  2.74it/s]
====train step of epoch 9 ==========
train_epoch9_loss: 0.6039814959719496
train_epoch9_accuracy: 0.6977827574006369
train_epoch9_precision: 0.6924047728315741
train_epoch9_recall: 0.7117584620827928
              precision    recall  f1-score   support

      NOTSEL       0.70      0.68      0.69      8479
      SELECT       0.69      0.71      0.70      8479

    accuracy                           0.70     16958
   macro avg       0.70      0.70      0.70     16958
weighted avg       0.70      0.70      0.70     16958

0.6977827574006369 0.6977827574006369 0.6977827574006369 0.6977827574006369
====validation step of epoch 9======================
3125it [03:17, 15.79it/s]
Answer-only score metric=(HU-meteor) level=5 : 0.12562669242186003
Answer-only score metric=(HU-meteor) level=10 : 0.15224304740530714
Answer-only score metric=(HU-meteor) level=50 : 0.21224406007529686
Answer-only score metric=(HU-meteor) level=100 : 0.23383160141118098
save to checkpoint: bert_weights/roberta_sentence_selector_ckpt_root_dir/RobertaCls_0518_22:24:57_sent_reranker
the testing checkpoint is RobertaCls_0518_22:24:57_sent_reranker
Traceback (most recent call last):
  File "/media/STORAGE/Shrikant/AVeriTeC_SKDU/src/my_methods/roberta_sentence_selector/train_roberta_sentence_selector.py", line 265, in <module>
    main()
  File "/media/STORAGE/Shrikant/AVeriTeC_SKDU/src/my_methods/roberta_sentence_selector/train_roberta_sentence_selector.py", line 194, in main
    import pred_sentence_scores
  File "/media/STORAGE/Shrikant/AVeriTeC_SKDU/src/my_methods/roberta_sentence_selector/pred_sentence_scores.py", line 5, in <module>
    from baseline.retriever.eval_sentence_retriever import eval_sentence_obj
ModuleNotFoundError: No module named 'baseline'
ERROR conda.cli.main_run:execute(49): `conda run python src/my_methods/roberta_sentence_selector/train_roberta_sentence_selector.py --data_dir data --bert_name roberta-base --max_epoch 10 --user_given_model_suffix sent_reranker` failed. (See above for error)

Process finished with exit code 1

